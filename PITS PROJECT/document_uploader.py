from global_settings import STORAGE_PATH, CACHE_FILE
from logging_functions import log_action
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.core.text_splitter import TokenTextSplitter
from llama_index.core.extractors import SummaryExtractor
from llama_index.embeddings.openai import OpenAIEmbedding

# main funciton that's responsible for handling the ingestion process. streamline & benefit from caching
def ingest_documents(): 
  # load all readable documents available in STORAGE_PATH
  documents = SimpleDirectoryReader(
    STORAGE_PATH,
    filename_as_id = True
  ).load_data()
  
  # for each document processed, a new event is stored in our log file using log_action from logging_functions.py
  for doc in documents: 
    print(doc.id_)
    log_action(
      f"File '{doc.id_}' uploaded user",
      action_type = "UPLOAD"
    )
  # the code checks if the cache file already exists and attempts to load it into memory. Remember,
  # the cache file contains the hashes and the results generated by the previous runs. The first time you
  # run the code, there will be no file, so the code wonâ€™t load any cached values.
  try:
    cached_hashes = IngestionCache.from_persist_path(
      CACHE_FILE
    )
    print("Cache file found. Running using cache...")
  except:
    cached_hashes = ""
    print("No cache file found. Running without...")
  # define and run the pipeline
  pipeline = IngestionPipeline (
    transformations = [
      ## basic chunking using TokenSplitter
      TokenTextSplitter(
        chunk_size = 1024,
        chunk_overlap = 20
      ),
      # metadata extractor that summarized each node
      SummaryExtractor (summaries = ['self']),
      OpenAIEmbedding()
    ],
    # Embedding generation using OpenAIEmbedding
    cache=cached_hashes
  )
  nodes = pipeline.run(documents=documents,show_progress=True)
  pipeline.cache.persist(CACHE_FILE)
  return nodes

if __name__ == "__main__":
  embedded_nodes = ingest_documents()    
  
  